
# DevPulseAI v3 â€” Autonomous Signal Intelligence Platform

> **Ingest. Analyze. Deliver.**
> Real-time technical intelligence powered by multi-agent AI and Model Context Protocol (MCP).

---

## âš¡ Overview

**DevPulseAI v3** is a cloud-native intelligence platform that autonomously aggregates signals from high-value developer sources, processes them through a multi-agent LLM pipeline, and delivers curated, actionable intelligence â€” via a real-time chat interface, REST API, or scheduled digest.

v3 introduces **MCP-first architecture**: four MCP servers (GitHub, HuggingFace, Supabase, Pinecone) provide structured, authenticated access to external services â€” replacing brittle REST scrapers with reliable, tool-backed integrations.

---

## ðŸ— Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       SIGNAL SOURCES                             â”‚
â”‚  GitHub Â· ArXiv Â· HackerNews Â· Medium Â· HuggingFace              â”‚
â”‚   (5 adapters, ~25 signals per cycle)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               MCP LAYER (Model Context Protocol)                 â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ GitHub   â”‚  â”‚ HuggingFace  â”‚  â”‚ Supabase â”‚  â”‚  Pinecone   â”‚  â”‚
â”‚  â”‚ MCP      â”‚  â”‚ MCP          â”‚  â”‚ MCP      â”‚  â”‚  MCP        â”‚  â”‚
â”‚  â”‚ Server   â”‚  â”‚ Server       â”‚  â”‚ Server   â”‚  â”‚  Server     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AGENT PIPELINE                                 â”‚
â”‚                                                                  â”‚
â”‚  Ingestion â†’ Normalization â†’ MultiSwarm Processing               â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚ Researcher  â”‚  â”‚  Analyst    â”‚  â”‚     Critic       â”‚         â”‚
â”‚  â”‚ (Summarize) â”‚  â”‚ (Score+Risk)â”‚  â”‚ (Verify+Refine)  â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚
                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     DELIVERY LAYER                                â”‚
â”‚                                                                  â”‚
â”‚  WebSocket Chat Â· REST API Â· Streamlit Dashboard Â· Email Digest   â”‚
â”‚  FastAPI (10 endpoints) Â· Proactive Recommendations               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ”Œ MCP Server Integration

DevPulseAI v3 uses **four MCP servers** as the backbone for external data access. Each serves a distinct purpose in the pipeline:

### GitHub MCP Server

| | |
|---|---|
| **Purpose** | Fetch trending repositories, search code, read file contents, manage issues/PRs |
| **Use in Pipeline** | The GitHub adapter queries trending repos created in the last 24 hours, extracts star counts, languages, and topics as signals. Also supports deep-dive repo analysis when users ask about specific repositories in the chat interface. |
| **Key Tools Used** | `search_repositories`, `get_file_contents`, `search_code` |
| **Auth** | `GITHUB_PERSONAL_ACCESS_TOKEN` |

### HuggingFace MCP Server

| | |
|---|---|
| **Purpose** | Search models, papers, datasets, and Spaces from the HuggingFace Hub |
| **Use in Pipeline** | Three signal streams: **(1)** Trending models with download counts, task types, and trending scores. **(2)** Daily research papers with author info and upvotes. **(3)** Popular Spaces with SDK info. Falls back to REST API if MCP is unavailable. |
| **Key Tools Used** | `hub_repo_search`, `paper_search`, `space_search` |
| **Auth** | `HUGGINGFACE_TOKEN` (optional, increases rate limits) |

### Supabase MCP Server

| | |
|---|---|
| **Purpose** | Persistent storage for all pipeline data â€” signals, intelligence, conversations, feedback |
| **Use in Pipeline** | Every signal flows through Supabase: raw signals are stored on ingestion, processed intelligence after agent analysis, conversation history for the chat interface, user feedback (ðŸ‘/ðŸ‘Ž) for quality tracking, and audit logs for observability. |
| **Tables** | `raw_signals` (347+), `processed_intelligence` (1028+), `conversations` (18+), `audit_logs` (7+), `agent_traces` (45+), `user_feedback` |
| **Auth** | `SUPABASE_ACCESS_TOKEN` |

### Pinecone MCP Server

| | |
|---|---|
| **Purpose** | Semantic vector search for intelligent recommendations and knowledge retrieval |
| **Use in Pipeline** | Stores processed intelligence as embeddings in the `devpulseai-knowledge` index (multilingual-e5-large). Powers the `/api/recommendations` endpoint â€” finds semantically similar signals to user queries. Enables the proactive recommendation engine that suggests relevant repos, papers, and topics based on conversation history. |
| **Key Tools Used** | `search-records`, `upsert-records`, `describe-index` |
| **Auth** | `PINECONE_API_KEY` |

---

## ðŸ“¡ Signal Sources & Adapters

| Source | Adapter | Data Fetched | Signals/Cycle |
|---|---|---|---|
| **GitHub** | `adapters/github.py` | Trending repos (24h), stars, languages, topics | ~5-10 |
| **ArXiv** | `adapters/arxiv.py` | Latest AI/ML papers, abstracts, authors | ~5-10 |
| **HackerNews** | `adapters/hackernews.py` | Top stories, points, comment counts | ~5-10 |
| **Medium** | `adapters/medium.py` | AI/ML engineering blogs (RSS feeds) | ~3-5 |
| **HuggingFace** | `adapters/huggingface.py` | Trending models, daily papers, popular Spaces | ~9-15 |

---

## ðŸš€ FastAPI Backend â€” 10 Endpoints

| Endpoint | Method | Description |
|---|---|---|
| `/ws/chat` | WebSocket | Real-time streaming chat with MultiSwarm agents |
| `/api/chat` | POST | REST fallback for chat (non-streaming) |
| `/api/feedback` | POST | Submit ðŸ‘/ðŸ‘Ž feedback â†’ Supabase `user_feedback` |
| `/api/signals` | GET | Query raw signals from Supabase |
| `/api/intelligence` | GET | Query processed intelligence |
| `/api/conversations` | GET | Retrieve conversation history |
| `/api/recommendations` | GET | Pinecone-powered proactive recommendations |
| `/ingest` | POST | Trigger signal ingestion from a specific source |
| `/daily-pulse` | POST | Run full pipeline: ingest all â†’ process â†’ store |
| `/ping` | GET | Health check for Render keep-alive |

---

## ðŸ“‹ v3 Phase Tracker

### âœ… Completed

| Phase | Features | Commit |
|---|---|---|
| **Phase 1 â€” Foundation** | Premium Streamlit UI (glassmorphism, dark theme), MultiSwarm agent system (Researcher, Analyst, Critic), Conversation pipeline, Signal/Intelligence models, Custom logger | `7e99c63` |
| **Phase 2 â€” MCP Integration** | Supabase MCP (6 tables + RLS policies), GitHub MCP (real API), Pinecone MCP (`devpulseai-knowledge` index, 4 records seeded), ArXiv fix (stopword stripping), Persistence client (datetime + conversation methods), End-to-end data flow verified | `d4d8d7b` |
| **Phase 3 â€” Backend + Features** | FastAPI `server.py` (10 endpoints), WebSocket chat, REST chat, Feedback â†’ Supabase, Proactive recommendation engine (Pinecone + Supabase), HuggingFace MCP adapter (models + papers + spaces), HackerNews adapter, Ingestion pipeline | `48b6f5a` |

### ðŸ”² Upcoming

| Phase | Features | Priority |
|---|---|---|
| **Phase 4 â€” Deployment** | Render deployment (Procfile, env config), Cron-job for `/daily-pulse`, Production health monitoring | ðŸ”´ High |
| **Phase 4 â€” Frontend** | React/Next.js UI connecting to FastAPI, WebSocket chat integration, Signal feed dashboard | ðŸ”´ High |
| **Phase 5 â€” Intelligence** | Pinecone auto-storage from conversations, Scheduled ingestion pipeline, Trend detection over time | ðŸŸ¡ Medium |
| **Phase 5 â€” Polish** | User authentication + sessions, Multi-provider LLM fallback chains, Email digest generation | ðŸŸ  Low |

---

## ðŸ›  Local Development

```bash
# Clone the repo
git clone https://github.com/STiFLeR7/DevPulseAIv2.git
cd DevPulseAIv2
git checkout feat/v3

# Install dependencies
pip install -r requirements.txt

# Set environment variables
export OPENAI_API_KEY=sk-...
export SUPABASE_URL=https://xxx.supabase.co
export SUPABASE_KEY=eyJ...
export PINECONE_API_KEY=pcsk_...

# Run FastAPI backend
uvicorn app.api.server:app --reload --port 8000

# Or run Streamlit UI
streamlit run app/ui/chat.py
```

---

## ðŸ“‚ Project Structure

```
DevPulseAIv2/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ adapters/          # Signal source adapters (GitHub, ArXiv, HF, HN, Medium)
â”‚   â”œâ”€â”€ agents/            # LLM agents (Researcher, Analyst, Critic)
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ main.py        # v2 FastAPI (legacy)
â”‚   â”‚   â””â”€â”€ server.py      # v3 FastAPI + WebSocket (10 endpoints)
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ swarm.py       # MultiSwarm orchestration
â”‚   â”‚   â”œâ”€â”€ conversation.py # Conversation pipeline
â”‚   â”‚   â”œâ”€â”€ recommendations.py # Pinecone + Supabase recommendation engine
â”‚   â”‚   â””â”€â”€ logger.py      # Structured logging
â”‚   â”œâ”€â”€ models/            # Pydantic models (Signal, Intelligence)
â”‚   â”œâ”€â”€ persistence/       # Supabase client + schema
â”‚   â”œâ”€â”€ reports/           # HTML email generation
â”‚   â””â”€â”€ ui/
â”‚       â””â”€â”€ chat.py        # Premium Streamlit chat interface
â”œâ”€â”€ scripts/               # Test & verification scripts
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ðŸ”‘ Environment Variables

| Variable | Required | Description |
|---|---|---|
| `OPENAI_API_KEY` | âœ… | LLM provider for agent pipeline |
| `SUPABASE_URL` | âœ… | Supabase project URL |
| `SUPABASE_KEY` | âœ… | Supabase service role key |
| `PINECONE_API_KEY` | âœ… | Pinecone vector search |
| `GITHUB_PERSONAL_ACCESS_TOKEN` | Optional | Higher rate limits for GitHub API |
| `HUGGINGFACE_TOKEN` | Optional | Higher rate limits for HuggingFace API |

---

> **Built with â¤ï¸ by Hill Patel.**
> *Powered by OpenAI Â· Supabase Â· Pinecone Â· MCP*
